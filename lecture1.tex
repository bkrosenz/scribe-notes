\documentclass[10pt]{article}
% Include statements
\usepackage{graphicx}
\usepackage{amsfonts,amssymb,amsmath,amsthm}
\usepackage[sort]{natbib}
\usepackage[margin=1in,nohead]{geometry}
\usepackage{multirow,rotating,array}
\usepackage{algorithm,algorithmic}
\usepackage{pdfsync}
\usepackage{hyperref}
\hypersetup{backref,colorlinks=true,citecolor=blue,linkcolor=blue,urlcolor=blue}
\renewcommand{\qedsymbol}{$\blacksquare$}
\setlength{\parindent}{0cm}
\setlength{\parskip}{10pt}


% For sequential numbering
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum\ -\ \arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}



% Theorem environments (\autoref compatible)
\usepackage{aliascnt}
\newtheorem{theorem}{Theorem}[lecnum]

\newaliascnt{result}{theorem}
\newtheorem{result}[theorem]{Result}
\aliascntresetthe{result}
\providecommand*{\resultautorefname}{Result}
\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\providecommand*{\lemmaautorefname}{Lemma}
\newaliascnt{prop}{theorem}
\newtheorem{proposition}[prop]{Proposition}
\aliascntresetthe{prop}
\providecommand*{\propautorefname}{Proposition}
\newaliascnt{cor}{theorem}
\newtheorem{corollary}[cor]{Corollary}
\aliascntresetthe{cor}
\providecommand*{\corautorefname}{Corollary}
\newaliascnt{conj}{theorem}
\newtheorem{conjecture}[conj]{Conjecture}
\aliascntresetthe{conj}
\providecommand*{\conjautorefname}{Corollary}
\newaliascnt{def}{theorem}
\newtheorem{definition}[def]{Definition}
\aliascntresetthe{def}
\providecommand*{\defautorefname}{Definition}
\newaliascnt{ex}{theorem}
\newtheorem{example}[ex]{Example}
\aliascntresetthe{ex}
\providecommand*{\exautorefname}{Example}


\newtheorem{assumption}{Assumption}
\renewcommand{\theassumption}{\Alph{assumption}}
\providecommand*{\assumptionautorefname}{Assumption}

\def\algorithmautorefname{Algorithm}
\renewcommand*{\figureautorefname}{Figure}%
\renewcommand*{\tableautorefname}{Table}%
\renewcommand*{\partautorefname}{Part}%
\renewcommand*{\chapterautorefname}{Chapter}%
\renewcommand*{\sectionautorefname}{Section}%
\renewcommand*{\subsectionautorefname}{Section}%
\renewcommand*{\subsubsectionautorefname}{Section}% 


% My Macros
\def\indep{\perp\!\!\!\perp}
\newcommand{\given}{\mbox{ }\vert\mbox{ }}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Expect}[1]{\mathbb{E}\!\left[#1\right]}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\B}{\mathcal{B}}
\DeclareMathOperator*{\Variance}{Var}
\newcommand{\Var}[1]{\Variance\!\left[#1\right]}
\DeclareMathOperator*{\Covariance}{Cov}
\newcommand{\Cov}[1]{\Covariance\!\left[#1\right]}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\email}[1]{\href{mailto:#1}{#1}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\indicator}{\mathbbm{1}}
\newcommand{\cdist}{\rightsquigarrow}
\newcommand{\cprob}{\xrightarrow{P}}
\newcommand{\clp}{\xrightarrow{L_p}}
\newcommand{\cas}{\xrightarrow{as}}
\renewcommand{\bar}{\overline}
\renewcommand{\hat}{\widehat}


% Your new macros


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To be entered
\setcounter{lecnum}{1}
\newcommand{\lecturer}{Prof.\ McDonald}
\newcommand{\scribe}{Prof.\ McDonald}
\newcommand{\chtitle}{Probability review I}
\newcommand{\lecdate}{22 August 2017}


\begin{document}
\rule{6.5in}{1pt}

\textsc{STAT--S 782
        \hfill \thelecnum\ --- \chtitle
        \hfill \lecdate}

\textsc{Lecturer: \lecturer \hfill Scribe: \scribe}
\rule{6.5in}{1pt}



\section*{Syllabus etc.}

\begin{itemize}
\item Respond to my {\tt Slack} invitation. Examine the board: \url{https://stat-s782fa2017.slack.com/}.
\item If you don't have a {\tt github} account, set one up:
  \url{https://education.github.com/pack}.
\item Consult the course website:
  \url{https://github.com/stats-782fa2017}.
\item Start thinking about a paper you want to present (due on
  Thursday, 24 August) and a group you want for your project.
\end{itemize}


\section{Random variables}
\label{sec:random-variables}

A {\em random variable} is a map $X$ from a probability space $\Omega$ to $\R$. We write
\begin{equation}
  P(X\in A)=P(\{\omega\in\Omega : X(\omega)\in A\})\label{eq:1}
\end{equation}
and we write $X \sim P$ to mean that $X$ has distribution $P$. 

The {\em cumulative distribution function} (cdf) of $X$ is
\begin{equation}
  F_X(x) = F(x) = P(X \leq x).\label{eq:2}
\end{equation}
If $X$ is discrete, its probability mass function (pmf) is
\begin{equation}
p_X(x) = p(x) = P(X = x).\label{eq:3}
\end{equation}
If $X$ is continuous, then its probability density function function (pdf) satisfies
\begin{equation}
P(X \in A) = \int_A p_X(x)dx = \int_A p(x)dx\label{eq:4}
\end{equation}
and $p_X(x) = p(x) = F'(x)$. The following are all equivalent:
\begin{align}
  X&\sim P,& X&\sim F, & X&\sim p, & \mathcal{L}(X)&=P.
\end{align}

Suppose that $X \sim P$ and $Y \sim Q$. We say that $X$ and $Y$ have
the same distribution if
\begin{equation}
  P(X\in A)=Q(Y \in A)\label{eq:5}
\end{equation}
for all $A$. In other words, $P = Q$. In that case we say that $X$ and
$Y$ are equal in distribution and we write $X \overset{d}{=} Y$ or
$\mathcal{L}(X) = \mathcal{L}(Y)$ . It can be shown that $X
\overset{d}{=} Y$ if and only if 
$F_X(t) = F_Y (t)$ for all $t$.

\section{Expected values}
\label{sec:expected-values}

The {\em mean} or expected value of $g(X)$ is
\begin{equation}
  \label{eq:6}
  \Expect{g(X)} = \int g(x)dF_X(x) = \int g(x) dP(x) = \begin{cases}
    \int_{-\infty}^{\infty} g(x)p(x)dx & \mbox{if $X$ is continuous}\\
    \sum_j g(x_j)p(x_j) & \mbox{if $X$ is discrete.}
  \end{cases}
\end{equation}

Recall the following useful properties of expectation:

\begin{enumerate}
\item $\Expect{\sum^k_{j=1} c_jg_j(X)} = \sum^k_{j=1} c_j\Expect{g_j(X)}.$
\item If $X_1, \ldots , X_n$ are independent then
    $\Expect{ \prod_{i=1}^n X_i } = \prod_i \Expect{X_i}$.
\item We often write $\mu = \Expect{X}$.
\item $\sigma^2 =\Var{X}=\Expect{(X-\mu)^2}$is the Variance.
\item $\Var{X}=\Expect{X^2}-\mu^2.$
\item If $X_1, \ldots , X_n$ are independent then
  $\Var{\sum_{i=1}^n a_i X_i} = \sum_i a_i^2\Var{X_i}$
\item The covariance is $\Cov{X,Y}=\Expect{(X-\mu_X)(Y -\mu_Y)}=\Expect{XY}-\mu_X\mu_Y$
and the correlation is $\rho(X,Y)=\Cov{X,Y}/\sigma_X\sigma_Y$. Recall
that $−1\leq \rho (X,Y)\leq1$.
\end{enumerate}

The conditional expectation of $Y$ given $X$ is the random variable
$\Expect{Y|X}$ whose valeu, when $X=x$ is $\Expect{Y|X=x} = \int y p(y|x) dy$
where $p(y|x) = p(x,y)/p(x)$. The {\em Law of Total Expectation} or
{\em Law of Iterated Expectation:}
\begin{equation}
  \label{eq:8}
  \Expect{Y} = \Expect{\Expect{Y|X}} = \int \Expect{Y|X=x}p_x(x)dx.
\end{equation}


\section{Important distributions}
\label{sec:import-distr}

\begin{description}
\item[Normal] $X\sim N(\mu,\sigma^2)$ if
  \begin{equation}
    \label{eq:7}
    p(x;\mu,\sigma^2) =
    \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\} 
  \end{equation}
\item[Multivariate normal] For $X\in \R^d$, $X\sim N_d(\mu, \Sigma)$ if
  \begin{equation}
    \label{eq:15}
    p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\exp\left\{-\frac{1}{2}(x-\mu)^\top\Sigma^{-1}(x-\mu)\right\} 
  \end{equation}
\item[Bernoulli] $X\sim Bernoulli(\theta)$ if $P(X=1)=\theta$ and
  $P(X=0) = 1-\theta$. Thus the pdf is
  \begin{equation}
    \label{eq:16}
    p(x;\theta) = \theta^x(1-\theta)^{1-x} I_{\{0,1\}}(x).
  \end{equation}
\item[Binomial] $X\sim Binomial(\theta)$ if
  \begin{equation}
    \label{eq:17}
    p(x;\theta) = P(X=x) = \binom{n}{x}\theta^x(1-\theta)^{n-x}I_{\{0,\ldots,n\}}(x).
  \end{equation}
\item[Uniform] $X\sim U(a,b)$ if $p(x) = I_{[a,b]}(x)/(b-a).$
\end{description}



\section{Convergence}
\label{sec:convergence}

Much of statistical machine learning is concerned with showing
asymptotic, or better yet, finite sample properties of computational
methods. We want to know whether our methods will perform well, and if
so how well will they perform relative to other methods. For that, we
need to discuss some useful results from probability theory. First,
we'll review the convergence of random variables.



Let $X_1,X_2,\ldots$ be a sequence of random variables, and let $X$ be
another random variable with distribution $P$. Let $F_n$ be the cdf of $X_n$ and let $F$ be
the cdf of $X$.
\begin{enumerate}
\item $X_n$ converges {\em almost surely} to $X$, $X_n\cas
  X$, if for every $\epsilon>0$,
  \begin{equation}
    \label{eq:1}
    P\left( \lim_{n\rightarrow\infty} |X_n-X| < \epsilon\right) =
    1. \footnote{The absolute value here can be replaced by any
      appropriate distance.}
  \end{equation}
\item $X_n$ converges {\em in probability} to $X$, $X_n\cprob
  X$, if for every $\epsilon>0$,
  \begin{equation}
    \label{eq:2}
   \lim_{n\rightarrow\infty} P\left(|X_n-X| < \epsilon\right) = 1. 
  \end{equation}
\item $X_n$ converges {\em in $L_p$} to $X$, $X_n\clp
  X$, if 
  \begin{equation}
   \lim_{n\rightarrow\infty} \int |X_n- X|^p dP =
   \lim_{n\rightarrow\infty} \Expect{|X_n-X|^p} = 0.
  \end{equation}
\item $X_n$ converges {\em in distribution} to $X$, $X_n \cdist
  X$, if 
  \begin{equation}
    lim_{n\rightarrow\infty} F_n(t) = F(t)
  \end{equation}
for all $t$ for which $F$ is continuous.
\end{enumerate}


\begin{example}
  This example shows that convergence in probability does not imply
  almost sure convergence. Let $S = [0, 1]$. Let $P$ be uniform on $[0,
  1]$. We draw $S ∼ P$ . Let $X(s) = s$ and let
  \begin{align*}
    X_1 &= s + I_{[0,1]}(s) & X_2 &= s + I_{[0,1/2]}(s) & X_3 &= s +
    I_{[1/2,1]}(s)\\
    X_4 &= s + I_{[0,1/3]}(s) & X_5 &= s + I_{[1/3,2/3]}(s) & X_6 &= s
    + I_{[2/3,1]}(s) 
  \end{align*}
etc. Then $X_n\cprob$ since $P(|X_n-X|>\epsilon)$ is equal to
the probability of an interval of $s$ values whose length is going to
zero. However, for every $s$, $X_n(s)$ alternates between the values
$s$ and $s+1$ infinitely often, so this convergence does not occur
almost surely.
\end{example}

\begin{theorem}
  \label{thm:convergence-1}
  The following relationships hold:
  \begin{enumerate}
  \item [(a)] $X_n\clp X$ implies that $X_n \cprob X$.
  \item [(b)]$X_n\cprob X$ implies that $X_n \cdist
    X$.
  \item [(c)] If $X_n\cdist X$ and if $P(X =c)=1$ for some
    real number $c$, then $X_n\cprob X$.
  \item [(d)]$X_n\cas X$ implies that $X_n \cprob X$.
  \end{enumerate}
\end{theorem}

\begin{theorem}
  \label{thm:convergence-2}
  Let $X_n,X,Y_n,Y$ be random variables. Let $g$ be a continuous
  function. Let $c$ be a constant.
  \begin{enumerate}
  \item [(a)] If $X_n\cprob X$ and $Y_n\cprob Y$, then
    $X_n+Y_n\cprob X+Y$.
  \item [(b)] If $X_n\clp X$ and $Y_n\clp Y$,
    then $X_n+Y_n\clp X+Y$.
  \item [(c)] If $X_n\cdist X$ and $Y_n\cdist c$,
    then $X_n+Y_n\cdist X+c$.
  \item [(d)] If $X_n\cprob X$ and $Y_n\cprob Y$, then
    $X_nY_n\cprob XY$.
  \item [(e)] If $X_n\cdist X$ and $Y_n\cdist c$, then
    $X_nY_n\cdist cX$.
  \item [(f)] If $X_n\xrightarrow{\mbox{converges somehow}} X$, then
    $g(X_n)\xrightarrow{\mbox{converges the same}} g(X)$.
  \end{enumerate}
  \begin{itemize}
  \item Parts (c) and (e) are known as {\em Slutsky's theorem}.
  \item Part (f) is known as {\em The continuous mapping theorem}.
  \end{itemize}
\end{theorem}

\section{LLNs and CLT}
\label{sec:llns-clt}

\begin{theorem}[Weak law of large numbers]
  \label{thm:llns-clt-1}
  Let $X_1, X_2,\ldots$ be independent random variables, each with
  finite mean and variance. Define $S_n = X_1 + \cdots + X_n$. Then
  $\frac{1}{n}(S_n-\Expect{S_n}) \cprob 0$.
\end{theorem}

\begin{theorem}[Strong law of large numbers]
  \label{thm:llns-clt-2}
  Let $X_1, X_2,\ldots$ be iid random variables with common mean $m$. Define $S_n = X_1 + \cdots + X_n$. Then
  $S_n/n \cas m$.
\end{theorem}

The laws of large numbers tell us that the probability mass of an
average of random variables ``piles up'' near its expectation. In just
a minute, we will see even more: how fast this piling occurs. But
first we should talk about the distribution of the average.

\begin{theorem}[Central limit theorem]
  \label{thm:llns-clt-3}
  Let $X_1, X_2,\ldots$ be iid with mean $\mu$ and variance
  $\sigma^2<\infty$. Let $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$. Then,
  \begin{equation}
    \label{eq:3}
    Z_n := \frac{\bar{X}_n - \mu}{\sqrt{\Var{\bar{X}_n}}} =
    \frac{\sqrt{n}(\bar{X}_n-\mu)}{\sigma}\cdist Z,
  \end{equation}
where $Z\sim N(0,1)$. 
\end{theorem}


% \bibliographystyle{scribebibsty}
% \bibliography{s782references}

\end{document}
