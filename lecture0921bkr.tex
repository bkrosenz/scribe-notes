\documentclass[10pt]{article}
% Include statements
\usepackage{graphicx}
\usepackage{amsfonts,amssymb,amsmath,amsthm}
\usepackage[sort]{natbib}
\usepackage[margin=1in,nohead]{geometry}
\usepackage{multirow,rotating,array}
\usepackage{algorithm,algorithmic}
\usepackage{pdfsync}
\usepackage{hyperref}
\hypersetup{backref,colorlinks=true,citecolor=blue,linkcolor=blue,urlcolor=blue}
\renewcommand{\qedsymbol}{$\blacksquare$}
\setlength{\parindent}{0cm}
\setlength{\parskip}{10pt}


% For sequential numbering
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum\ -\ \arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}



% Theorem environments (\autoref compatible)
\usepackage{aliascnt}
\newtheorem{theorem}{Theorem}[lecnum]

\newaliascnt{result}{theorem}
\newtheorem{result}[theorem]{Result}
\aliascntresetthe{result}
\providecommand*{\resultautorefname}{Result}
\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\providecommand*{\lemmaautorefname}{Lemma}
\newaliascnt{prop}{theorem}
\newtheorem{proposition}[prop]{Proposition}
\aliascntresetthe{prop}
\providecommand*{\propautorefname}{Proposition}
\newaliascnt{cor}{theorem}
\newtheorem{corollary}[cor]{Corollary}
\aliascntresetthe{cor}
\providecommand*{\corautorefname}{Corollary}
\newaliascnt{conj}{theorem}
\newtheorem{conjecture}[conj]{Conjecture}
\aliascntresetthe{conj}
\providecommand*{\conjautorefname}{Corollary}
\newaliascnt{def}{theorem}
\newtheorem{definition}[def]{Definition}
\aliascntresetthe{def}
\providecommand*{\defautorefname}{Definition}
\newaliascnt{ex}{theorem}
\newtheorem{example}[ex]{Example}
\aliascntresetthe{ex}
\providecommand*{\exautorefname}{Example}


\newtheorem{assumption}{Assumption}
\renewcommand{\theassumption}{\Alph{assumption}}
\providecommand*{\assumptionautorefname}{Assumption}

\def\algorithmautorefname{Algorithm}
\renewcommand*{\figureautorefname}{Figure}%
\renewcommand*{\tableautorefname}{Table}%
\renewcommand*{\partautorefname}{Part}%
\renewcommand*{\chapterautorefname}{Chapter}%
\renewcommand*{\sectionautorefname}{Section}%
\renewcommand*{\subsectionautorefname}{Section}%
\renewcommand*{\subsubsectionautorefname}{Section}% 


% My Macros
\def\indep{\perp\!\!\!\perp}
\newcommand{\given}{\mbox{ }\vert\mbox{ }}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Expect}[1]{\mathbb{E}\!\left[#1\right]}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\B}{\mathcal{B}}
\DeclareMathOperator*{\Variance}{Var}
\newcommand{\Var}[1]{\Variance\!\left[#1\right]}
\DeclareMathOperator*{\Covariance}{Cov}
\newcommand{\Cov}[1]{\Covariance\!\left[#1\right]}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\email}[1]{\href{mailto:#1}{#1}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\indicator}{\mathbbm{1}}
\newcommand{\cdist}{\rightsquigarrow}
\newcommand{\cprob}{\xrightarrow{P}}
\newcommand{\clp}{\xrightarrow{L_p}}
\newcommand{\cas}{\xrightarrow{as}}
\renewcommand{\bar}{\overline}
\renewcommand{\hat}{\widehat}


% Your new macros



% To be entered
\setcounter{lecnum}{0}
\newcommand{\lecturer}{Prof.\ McDonald}
\newcommand{\scribe}{Prof.\ McDonald}
\newcommand{\chtitle}{Scribe template}
\newcommand{\lecdate}{7 August 2017}


\begin{document}
\rule{6.5in}{1pt}

\textsc{STAT--S 782
        \hfill \thelecnum\ --- \chtitle
        \hfill \lecdate}

\textsc{Lecturer: \lecturer \hfill Scribe: \scribe}
\rule{6.5in}{1pt}

9 
\section*{Specialized techniques}
\subsection{Dual (sub)gradient methods}
min_xf(x) \st Ax=b \then dual is max_u -f^*(-A^Tu)-b^Tu.
let g(u):=f^*(-A^Tu)-b^Tu. goal: minimize g(u)
subdiff \partial g(u)= A\partial f^*(-A^Tu)-b^Tu, but if x\in\argmin_z L(z,u) \then \partial g(u)=Ax-b
Therefore we may solve as follows:
guess u^(0) and iterate
x^(k)\in\argmin_xf(x)+(u^(k-1))^TAx\\
u^(k) = k^(k-1)+t_k(Ax^(k-1)-b)\\

formally: if f is strictly convex \then
\begin{itemize}
\item conjugate f* is diffable
\item procedure is dual Gradient Ascent
\item x^(k) is unique argmin
\end{itemize}

We can choose t_k as before and apply proximal methods, or acceleration.

\subsection{Decomposable Dual}
\begin{example}
  \[ min_x \sum_{i=1}^B f_i(x_i) \st Ax=b \]
  standard minimization decomposes into x^+\in\argmin_x\sum_{i=1}^B f_i(x_i) + u^TAx, which is equivalent to solving separately for each x_i; x^+_i\in\argmin_{x_i}f_i(x_i)+u^TA_ix_i.
  
  iterate:
x_i^(k)\in\argmin_x_if(x_i)+(u^(k-1))^TA_ix_i\\ 
u^(k) = k^(k-1)+t_k(\sum_{i=1}^B A_ix_i^{(k)}-b )

(strong duality holds in this example since we have no inquality constraints)

one variation:
if the constraints are Ax\leq b, we make a slight modification to u^(k)
u^(k) = (k^(k-1)+t_k(\sum_{i=1}^B A_ix_i^{(k)}-b ))_+
\end{example}

\subsection{Augmented Lagrangian}
need some constraints on f for dual ascent to work (\to g^*), which the Augmented Lagrangian provides.  Some simple sufficient conditions are:
\begin{enumerate}
\item f is strongly convex \then for accuracy \epsilon we require \O(1/\epsilon) iterations
\item f is strongly convex and \nabla f Lipschitz \then\O(log(1/\epsilon)) iterations
  \end{enumerate}
    Note: To get strong duality (primal optimality) must still also satisfy the conditions mentioned earlier (e.g. Slater's condition).

    Transform \min_xf(x)+\rho/2||Ax-b||_2^2 \st Ax=b  objective is strongly convex if A has full column rank.  Dual gradient ascent becomes
    x^(k)=\argmin_xf(x)+(u^(k-1))^TAx+\rho/2||Ax-b||_2^2\\ % second term forces us toward feasible x
    u^(k) = k^(k-1)+\rho(Ax^(k-1)-b)\\
    Replacing the step size t_k with \rho gives better convergence properties than the original DGA.  But by introducing the norm we lose the decomposable property (if we had it) and attendant parallelism.
    rho balances primal feasibility with small objective; larger rho, then closer to primal feasible points, less weight on objective value.
    
\subsection{alternating direction method of multipliers (ADMM)}
Fixes the augmented lagrangian
\[min_{x,z}f(x)+g(x) \st Ax+Bz=c\]
Augment: add $\rho/2||Ax+Bz-c||_2^2$ to the objective (penalizing unfeasibility)
L_\pho(x,z,u)=f(x)+g(z)+u^T(Ax+Bz-c) + \rho/2||Ax+Bz-c||_2^2

iterate
x^(k)=\argmin_xL_\pho(x,z^(k-1),u^(k-1))\\
z^(k)=\argmin_zL_\pho(x^(k-1),z,u^(k-1))
u^(k) = k^(k-1)+\rho(Ax^(k-1)+Bx^(k-1)-b)\\

properties (some of which do not require A and B to be full rank):
Ax^(k)+Bz^(k)-c\to 0 as k\to\infty (and get primal feasibility)
f^(k)+g^(k)\to f^*+g^* (primal optimality)
u^(k)\to u^*(dual solution)
(don't necessarily get x^(k)\to x* and z^(k)\to z^*)

the convergence rate is unknown, but empirically is close to \O(1/\epsilon)

\begin{example}[LASSO]
  \min_\beta 1/2||y+X\beta||_2^2+\lambda||\alpha|| \st \alpha=\beta
  ADMM
  \beta^(k)=(X^TX+\rho I)^-1(X^Ty+\rho(\alpha^(k-1)-w^(k-1))) % looks like ridge regression
  \alpha^(k)=S_{\lambda/\rho}(\beta^(k)+w^(k-1))
  w^(k)=w^(k-1)+\beta^(k)-\alpha^(k)

  issues: how to choose \rho.  different ADMM formulations of the problem may have different convergence properties.
\end{example}

\subsection{Consensus ADMM}
min_x\sum_i=1^Bf_i(a_i^Tx+b_i)+g(x)
introduce blocks of RVs x_1,...,x_B and minimize \min_{x1,...,xB,x}sum_i=1^Bf_i(a_i^Tx+b_i)+g(x)\st x_i=x\forall i
Now x_i^(k)=\argmin_{x_i}f_i(a_i^Tx_i+b_i)+\rho/2||x_i-x^(k-1)+w_i^(k-1)||_2^2
x^(k)=\argmin_{x}\rho/2||x-\overline{x}^(k)+w_i^(k-1)||_2^2 + g(x)
w_i^(k)=w_i^(k-1)+x_i^(k)-x^(k)

\subsection{Coordinate Descent}
Works well with LASSO and other stats opers
If f(x)=g(x)+\sum_i=1^nh_i(x_i) where g is conv and diff, h merely convex
\then we can:
guess x^(0)
iterate
update x_1^(k)\in argmin_x_1 f(x_1,x_2^(k-1),...,x_n^(k-1))
x_2^(k)\in argmin_x_2 f(x_1^(k),x_2,...,x_n^(k-1)) (minimize over whole vector or block)

\begin{example}[LASSO]
  (state of the art LASSO software)
  ||beta||=\sum_i=1^P|\beta_i|
  \beta_i=S_{\lambda/||x_i||_2^2}(\frac{X_i^T(y-X_{-i}\beta_{-i})}{X_i^TX_i})
  just take derivative obj \wrt \beta_i
\end{example}


This document provides some information for creating the scribe files
for STAT--S 682. Before you begin, please read through the macros and
commands above. You should change all the {\tt to be entered} commands
as appropriate. Then, simply begin your notes in this document. 

The first thing to note is that there are a number of
macros which I have already defined. You should use these as much as
possible. These include macros for $\R$, $\Expect{}$, $\Var{}$, and
many others. Whenever you find yourself needing these symbols, please
use the macros.

Second, please place your newly defined macros in the appropriate
location in the header.

Theorems, definitions, conjectures, corollaries, etc have already been
created using \AmS-\LaTeX.

All displayed equations should be numbered for further reference. 

A few notes about references: try to use the \verb|\autoref| rather
that \verb|\ref|. That way you can type \\ \verb|\autoref{thm:whatever}|
to get \autoref{thm:convergence-1} rather than writing \verb|Theorem \ref{thm:whatever}| 
to get Theorem \ref{thm:convergence-1}. 

Finally, please use \textsc{Bib}\TeX to create references. Simply add
new references to the {\tt .bib} file in the repo. Also, I recommend
using the {\tt natbib} package which gives, for instance,
\citep{Vapnik1998} or~\citet{Vapnik1998} rather than~\cite{Vapnik1998}

The rest of this document is an example from an old course. Note
especially~\autoref{alg:kalman}.

\section{Random variables}


A {\em random variable} is a map $X$ from a probability space $\Omega$ to $\R$. We write
\begin{equation}
  P(X\in A)=P(\{\omega\in\Omega : X(\omega)\in A\})\label{eq:1}
\end{equation}
and we write $X \sim P$ to mean that $X$ has distribution $P$. 

The {\em cumulative distribution function} (cdf) of $X$ is
\begin{equation}
  F_X(x) = F(x) = P(X \leq x).\label{eq:2}
\end{equation}
If $X$ is discrete, its probability mass function (pmf) is
\begin{equation}
p_X(x) = p(x) = P(X = x).\label{eq:3}
\end{equation}
If $X$ is continuous, then its probability density function function (pdf) satisfies
\begin{equation}
P(X \in A) = \int_A p_X(x)dx = \int_A p(x)dx\label{eq:4}
\end{equation}
and $p_X(x) = p(x) = F'(x)$. The following are all equivalent:
\begin{align}
  X&\sim P,& X&\sim F, & X&\sim p, & \mathcal{L}(X)&=P.
\end{align}

Suppose that $X \sim P$ and $Y \sim Q$. We say that $X$ and $Y$ have
the same distribution if
\begin{equation}
  P(X\in A)=Q(Y \in A)\label{eq:5}
\end{equation}
for all $A$. In other words, $P = Q$. In that case we say that $X$ and
$Y$ are equal in distribution and we write $X \overset{d}{=} Y$ or
$\mathcal{L}(X) = \mathcal{L}(Y)$ . It can be shown that $X
\overset{d}{=} Y$ if and only if 
$F_X(t) = F_Y (t)$ for all $t$.

\section{Expected values}
\label{sec:expected-values}

The {\em mean} or expected value of $g(X)$ is
\begin{equation}
  \label{eq:6}
  \Expect{g(X)} = \int g(x)dF_X(x) = \int g(x) dP(x) = \begin{cases}
    \int_{-\infty}^{\infty} g(x)p(x)dx & \mbox{if $X$ is continuous}\\
    \sum_j g(x_j)p(x_j) & \mbox{if $X$ is discrete.}
  \end{cases}
\end{equation}

Recall the following useful properties of expectation:

\begin{enumerate}
\item $\Expect{\sum^k_{j=1} c_jg_j(X)} = \sum^k_{j=1} c_j\Expect{g_j(X)}.$
\item If $X_1, \ldots , X_n$ are independent then
    $\Expect{ \prod_{i=1}^n X_i } = \prod_i \Expect{X_i}$.
\item We often write $\mu = \Expect{X}$.
\item $\sigma^2 =\Var{X}=\Expect{(X-\mu)^2}$is the Variance.
\item $\Var{X}=\Expect{X^2}-\mu^2.$
\item If $X_1, \ldots , X_n$ are independent then
  $\Var{\sum_{i=1}^n a_i X_i} = \sum_i a_i^2\Var{X_i}$
\item The covariance is $\Cov{X,Y}=\Expect{(X-\mu_X)(Y -\mu_Y)}=\Expect{XY}-\mu_X\mu_Y$
and the correlation is $\rho(X,Y)=\Cov{X,Y}/\sigma_X\sigma_Y$. Recall
that $−1\leq \rho (X,Y)\leq1$.
\end{enumerate}

The conditional expectation of $Y$ given $X$ is the random variable
$\Expect{Y|X}$ whose valeu, when $X=x$ is $\Expect{Y|X=x} = \int y p(y|x) dy$
where $p(y|x) = p(x,y)/p(x)$. The {\em Law of Total Expectation} or
{\em Law of Iterated Expectation:}
\begin{equation}
  \label{eq:8}
  \Expect{Y} = \Expect{\Expect{Y|X}} = \int \Expect{Y|X=x}p_x(x)dx.
\end{equation}


\begin{algorithm}[t!]
  \caption{Approximate Kalman filter\label{alg:kalman}}
  \begin{algorithmic}[1]
  \STATE {\bfseries Input:}
  % Produce minimum MSE estimates of
  % $w_i$ $i=1,\ldots,T$ using equation
  % \eqref{eq:state-space-model2}\\
  Initial state $w_0$, state variance $\mathbf{P}_0$,
    and sketching matrices $\Pi\in\R^{n\times m}$ and
    $\mathbf{R}\in\R^{p\times q}$.
  \STATE {\bfseries Compress:} $\tilde{\mathbf{H}}\rightarrow\Pi^\top
  \mathbf{H} \Pi$, $\tilde{\mathbf{G}}\rightarrow \mathbf{R}^\top
  \mathbf{G}\mathbf{R}$, $\tilde{\X}\rightarrow \Pi\X\mathbf{R}$,
  $\tilde{\mathbf{A}} = \mathbf{R}^\top \mathbf{A} \mathbf{R}$,
  $\tilde{\mathbf{P}}_0 = \mathbf{R}^\top \mathbf{P}_0\mathbf{R}$,
  $z_0=\mathbf{R}w_0$. \\ 
  \FOR{$i=1$ {\bfseries to} $T$}
    \STATE Compress the data $\tilde{Y}_i=\Pi Y_i$
    \STATE Run the standard Kalman filter to produce:
    $
    \tilde{z}_{i} \leftarrow
    \tilde{\mathbf{A}}\tilde{z}_{i-1}+\tilde{\mathbf{K}}_i(\tilde{Y}_i -
    \tilde{\X}\tilde{\mathbf{A}}\tilde{z}_{i-1}) 
    $
    \STATE Update 
    $\tilde{w}_i \leftarrow \mathbf{R}\tilde{z}_i$ and increment $\mathbf{P}_{i} = 
    \mathbf{R}\tilde{\mathbf{P}}_{i-1} \mathbf{R}^\top$\;
  \ENDFOR
  \STATE Return $w_i$ and $\mathbf{P}_i$, $i=1,\ldots,T$.
\end{algorithmic}
\end{algorithm}


\section{Important distributions}
\label{sec:import-distr}

\begin{description}
\item[Normal] $X\sim N(\mu,\sigma^2)$ if
  \begin{equation}
    \label{eq:7}
    p(x;\mu,\sigma^2) =
    \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\} 
  \end{equation}
\item[Multivariate normal] For $X\in \R^d$, $X\sim N_d(\mu, \Sigma)$ if
  \begin{equation}
    \label{eq:15}
    p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\exp\left\{-\frac{1}{2}(x-\mu)^\top\Sigma^{-1}(x-\mu)\right\} 
  \end{equation}
\item[Bernoulli] $X\sim Bernoulli(\theta)$ if $P(X=1)=\theta$ and
  $P(X=0) = 1-\theta$. Thus the pdf is
  \begin{equation}
    \label{eq:16}
    p(x;\theta) = \theta^x(1-\theta)^{1-x} I_{\{0,1\}}(x).
  \end{equation}
\item[Binomial] $X\sim Binomial(\theta)$ if
  \begin{equation}
    \label{eq:17}
    p(x;\theta) = P(X=x) = \binom{n}{x}\theta^x(1-\theta)^{n-x}I_{\{0,\ldots,n\}}(x).
  \end{equation}
\item[Uniform] $X\sim U(a,b)$ if $p(x) = I_{[a,b]}(x)/(b-a).$
\end{description}


Much of statistical machine learning is concerned with showing
asymptotic, or better yet, finite sample properties of computational
methods. We want to know whether our methods will perform well, and if
so how well will they perform relative to other methods. For that, we
need to discuss some useful results from probability theory. First,
we'll review the convergence of random variables.

\section{Convergence}
\label{sec:convergence}

Let $X_1,X_2,\ldots$ be a sequence of random variables, and let $X$ be
another random variable with distribution $P$. Let $F_n$ be the cdf of $X_n$ and let $F$ be
the cdf of $X$.
\begin{enumerate}
\item $X_n$ converges {\em almost surely} to $X$, $X_n\cas
  X$, if for every $\epsilon>0$,
  \begin{equation}
    \label{eq:1}
    P\left( \lim_{n\rightarrow\infty} |X_n-X| < \epsilon\right) =
    1. \footnote{The absolute value here can be replaced by any
      appropriate distance.}
  \end{equation}
\item $X_n$ converges {\em in probability} to $X$, $X_n\cprob
  X$, if for every $\epsilon>0$,
  \begin{equation}
    \label{eq:2}
   \lim_{n\rightarrow\infty} P\left(|X_n-X| < \epsilon\right) = 1. 
  \end{equation}
\item $X_n$ converges {\em in $L_p$} to $X$, $X_n\clp
  X$, if 
  \begin{equation}
   \lim_{n\rightarrow\infty} \int |X_n- X|^p dP =
   \lim_{n\rightarrow\infty} \Expect{|X_n-X|^p} = 0.
  \end{equation}
\item $X_n$ converges {\em in distribution} to $X$, $X_n \cdist
  X$, if 
  \begin{equation}
    lim_{n\rightarrow\infty} F_n(t) = F(t)
  \end{equation}
for all $t$ for which $F$ is continuous.
\end{enumerate}


\begin{example}
  This example shows that convergence in probability does not imply
  almost sure convergence. Let $S = [0, 1]$. Let $P$ be uniform on $[0,
  1]$. We draw $S ∼ P$ . Let $X(s) = s$ and let
  \begin{align*}
    X_1 &= s + I_{[0,1]}(s) & X_2 &= s + I_{[0,1/2]}(s) & X_3 &= s +
    I_{[1/2,1]}(s)\\
    X_4 &= s + I_{[0,1/3]}(s) & X_5 &= s + I_{[1/3,2/3]}(s) & X_6 &= s
    + I_{[2/3,1]}(s) 
  \end{align*}
etc. Then $X_n\cprob$ since $P(|X_n-X|>\epsilon)$ is equal to
the probability of an interval of $s$ values whose length is going to
zero. However, for every $s$, $X_n(s)$ alternates between the values
$s$ and $s+1$ infinitely often, so this convergence does not occur
almost surely.
\end{example}

\begin{theorem}
  \label{thm:convergence-1}
  The following relationships hold:
  \begin{enumerate}
  \item [(a)] $X_n\clp X$ implies that $X_n \cprob X$.
  \item [(b)]$X_n\cprob X$ implies that $X_n \cdist
    X$.
  \item [(c)] If $X_n\cdist X$ and if $P(X =c)=1$ for some
    real number $c$, then $X_n\cprob X$.
  \item [(d)]$X_n\cas X$ implies that $X_n \cprob X$.
  \end{enumerate}
\end{theorem}

\begin{theorem}
  \label{thm:convergence-2}
  Let $X_n,X,Y_n,Y$ be random variables. Let $g$ be a continuous
  function. Let $c$ be a constant.
  \begin{enumerate}
  \item [(a)] If $X_n\cprob X$ and $Y_n\cprob Y$, then
    $X_n+Y_n\cprob X+Y$.
  \item [(b)] If $X_n\clp X$ and $Y_n\clp Y$,
    then $X_n+Y_n\clp X+Y$.
  \item [(c)] If $X_n\cdist X$ and $Y_n\cdist c$,
    then $X_n+Y_n\cdist X+c$.
  \item [(d)] If $X_n\cprob X$ and $Y_n\cprob Y$, then
    $X_nY_n\cprob XY$.
  \item [(e)] If $X_n\cdist X$ and $Y_n\cdist c$, then
    $X_nY_n\cdist cX$.
  \item [(f)] If $X_n\xrightarrow{\mbox{converges somehow}} X$, then
    $g(X_n)\xrightarrow{\mbox{converges the same}} g(X)$.
  \end{enumerate}
  \begin{itemize}
  \item Parts (c) and (e) are known as {\em Slutsky's theorem}.
  \item Part (f) is known as {\em The continuous mapping theorem}.
  \end{itemize}
\end{theorem}

\section{LLNs and CLT}
\label{sec:llns-clt}

\begin{theorem}[Weak law of large numbers]
  \label{thm:llns-clt-1}
  Let $X_1, X_2,\ldots$ be independent random variables, each with
  finite mean and variance. Define $S_n = X_1 + \cdots + X_n$. Then
  $\frac{1}{n}(S_n-\Expect{S_n}) \cprob 0$.
\end{theorem}

\begin{theorem}[Strong law of large numbers]
  \label{thm:llns-clt-2}
  Let $X_1, X_2,\ldots$ be iid random variables with common mean $m$. Define $S_n = X_1 + \cdots + X_n$. Then
  $S_n/n \cas m$.
\end{theorem}

The laws of large numbers tell us that the probability mass of an
average of random variables ``piles up'' near its expectation. In just
a minute, we will see even more: how fast this piling occurs. But
first we should talk about the distribution of the average.

\begin{theorem}[Central limit theorem]
  \label{thm:llns-clt-3}
  Let $X_1, X_2,\ldots$ be iid with mean $\mu$ and variance
  $\sigma^2<\infty$. Let $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$. Then,
  \begin{equation}
    \label{eq:3}
    Z_n := \frac{\bar{X}_n - \mu}{\sqrt{\Var{\bar{X}_n}}} =
    \frac{\sqrt{n}(\bar{X}_n-\mu)}{\sigma}\cdist Z,
  \end{equation}
where $Z\sim N(0,1)$. 
\end{theorem}


\bibliographystyle{scribebibsty}
\bibliography{s782references}

\end{document}
