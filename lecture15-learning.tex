\documentclass[10pt]{article}
% Include statements
\usepackage{graphicx}
\usepackage{amsfonts,amssymb,amsmath,amsthm}
\usepackage[sort]{natbib}
\usepackage[margin=1in,nohead]{geometry}
\usepackage{multirow,rotating,array}
\usepackage{algorithm,algorithmic}
\usepackage{pdfsync}
\usepackage{hyperref}
\usepackage{csquotes}
\hypersetup{backref,colorlinks=true,citecolor=blue,linkcolor=blue,urlcolor=blue}
\renewcommand{\qedsymbol}{$\blacksquare$}
\setlength{\parindent}{0cm}
\setlength{\parskip}{10pt}


% For sequential numbering
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum\ -\ \arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}



% Theorem environments (\autoref compatible)
\usepackage{aliascnt}
\newtheorem{theorem}{Theorem}[lecnum]

\newaliascnt{result}{theorem}
\newtheorem{result}[theorem]{Result}
\aliascntresetthe{result}
\providecommand*{\resultautorefname}{Result}
\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\providecommand*{\lemmaautorefname}{Lemma}
\newaliascnt{prop}{theorem}
\newtheorem{proposition}[prop]{Proposition}
\aliascntresetthe{prop}
\providecommand*{\propautorefname}{Proposition}
\newaliascnt{cor}{theorem}
\newtheorem{corollary}[cor]{Corollary}
\aliascntresetthe{cor}
\providecommand*{\corautorefname}{Corollary}
\newaliascnt{conj}{theorem}
\newtheorem{conjecture}[conj]{Conjecture}
\aliascntresetthe{conj}
\providecommand*{\conjautorefname}{Corollary}
\newaliascnt{def}{theorem}
\newtheorem{definition}[def]{Definition}
\aliascntresetthe{def}
\providecommand*{\defautorefname}{Definition}
\newaliascnt{ex}{theorem}
\newtheorem{example}[ex]{Example}
\aliascntresetthe{ex}
\providecommand*{\exautorefname}{Example}


\newtheorem{assumption}{Assumption}
\renewcommand{\theassumption}{\Alph{assumption}}
\providecommand*{\assumptionautorefname}{Assumption}

\def\algorithmautorefname{Algorithm}
\renewcommand*{\figureautorefname}{Figure}%
\renewcommand*{\tableautorefname}{Table}%
\renewcommand*{\partautorefname}{Part}%
\renewcommand*{\chapterautorefname}{Chapter}%
\renewcommand*{\sectionautorefname}{Section}%
\renewcommand*{\subsectionautorefname}{Section}%
\renewcommand*{\subsubsectionautorefname}{Section}% 


% My Macros
\def\indep{\perp\!\!\!\perp}
\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%
\newcommand{\given}{\mbox{ }\vert\mbox{ }}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Expect}[1]{\mathbb{E}\!\left[#1\right]}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\B}{\mathcal{B}}
\DeclareMathOperator*{\Variance}{Var}
\newcommand{\Var}[1]{\Variance\!\left[#1\right]}
\DeclareMathOperator*{\Covariance}{Cov}
\newcommand{\Cov}[1]{\Covariance\!\left[#1\right]}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\email}[1]{\href{mailto:#1}{#1}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\indicator}{\mathbbm{1}}
\newcommand{\cdist}{\rightsquigarrow}
\newcommand{\cprob}{\xrightarrow{P}}
\newcommand{\clp}{\xrightarrow{L_p}}
\newcommand{\cas}{\xrightarrow{as}}
\renewcommand{\bar}{\overline}
\renewcommand{\hat}{\widehat}


% Your new macros



% To be entered
\setcounter{lecnum}{15}
\newcommand{\lecturer}{Prof.\ McDonald}
\newcommand{\scribe}{Kiran\ Kumar}
\newcommand{\chtitle}{Introduction to learning - 2}
\newcommand{\lecdate}{12 October 2017}


\begin{document}
\rule{6.5in}{1pt}

\textsc{STAT--S 782
        \hfill \thelecnum\ --- \chtitle
        \hfill \lecdate}

\textsc{Lecturer: \lecturer \hfill Scribe: \scribe}
\rule{6.5in}{1pt}


\section*{Other types of learning}

\begin{itemize}
    \item{Agnostic learning (model-free)} 
    \item{Adversarial learning}
\end{itemize}
In this scenario the target $C^*(or\ f^*)$ may not be in $C(or \mathfrak{F})$\\

The setup for learning involves the following components

\begin{itemize}
    \item{Sets x,y,u}
    \item{The distribution $\mathcal{P}$ on $x\times y$}
    \item{The function class $\mathfrak{F}(or\ C) = \{f:x \rightarrow u\}$}
    \item{loss $l:y\times y \rightarrow [0,1] $}
\end{itemize}

The process for learning generally involves the following steps

\begin{enumerate}
    \item{Get sample iid $Z^n = (z_1, \ldots, z_n)$; \quad $z_i \distas{iid} p \in \mathcal{P}$}
    \item{Let Algorithm $\mathcal{A} = \{A_n\}_{n=1}^\infty, \quad A_n:Z^n  \rightarrow \mathfrak{F}$ }
    \item{Form a hypothesis $\hat{f}_n = A_n(Z^n) = A_n\big( (x_1,y_1), \ldots, (x_n,y_n) \big)$}
    \item{Find expected loss of the hypothesis $L_p(\hat{f}_n) = \Expect{l(y^{n+1}, \hat{f}(x^{n+1}) \given Z^n}$}
    \item{Then the \enquote{best prediction} is given by $L_p^*(\mathfrak{F}) = \inf\limits_{f\in \mathfrak{F}} L_p(f); \quad 0\leq L_p^*(\mathfrak{F}\leq L_p(\hat{f}_n\leq 1$}
\end{enumerate}

We can then define risk as follow

\begin{align*}
    r_A(n,\epsilon) = \sup\limits_{p \in \mathcal{P}} p^n(Z^n:L_p(\hat{f}_n \geq L_p^*(\mathfrak{F}+\epsilon)
\end{align*}

This is the worst loss over data set.

\section*{Example: Noisy Classification}
We have $\mathcal{X}$, $p_{x} \in \mathcal{P}_{\mathcal{X}}$, $C^*,C$

\begin{itemize}
    \item{$X^n &= (x_1, \ldota, x_n) \distas{iid} p_x$}
    \item{The classification happens with a probability, $y_i= 
    \begin{cases}
        I(x_i \in C^*), &w.p.\quad 1-\eta\\
        1-I(x_i \in C^*), &w.p.\quad \eta
    \end{cases}$}
    \item{$\eta < \frac{1}{2}$  is the noise rate}
    \item{$y=u=\{0,1\}, \quad \mathfrak{F} = \{I_c: c\in \mathcal{C}\}$}
    \item{A loss function $l(y,u) &= \lvert y-u \rvert^2$}
    \item{Set of probabilites $\{p_{x,c}:p_x\in \mathcal{P}_x, c \in \mathcal{C}\}$}
\end{itemize}

\begin{align*}
    p_{y\given x,c}(1 \given X=x,c) &= (1-\eta) I(x \in c) + \eta I(x \in c^\complement)\\
    & = (1-\eta) I(x \in c) + \eta (1-I(x \in c))
\end{align*}

\begin{align*}
    p_{y\given x,c}(0 \given X=x,c) &= 1 - P_{y\given x,c}(1 \given X=x,c)\\
    &= \eta I(x\in c) + (1-\eta)  (1-I(x \in c))
\end{align*}

So, for any $A \subseteq x$ 

\begin{align*}
    p_{x,c}(A \times {1}) &= \int_A p_{y \given x,c}(1 \given X=x) \quad p_x(dx)\\
    &= \int_A (1-\eta) I(x \in C) + \eta (1- I(x \in c)) \quad p_x(dx)\\
    \ldots &= \eta p_x(A) + (1-2\eta) p_x(A \cap c)
\end{align*}

Similarly,

\begin{align*}
    p_{x,c}(A \times {0}) &= (1-\eta) p_x(A) - (1-2\eta) p_x(A \cap c)
\end{align*}

The loss of some hypothesis $c'$ for any c can be written as 

\begin{align*}
    L_{p_{x,c}}(I_{c'}) &= \int_{x \times \left\{0,1\right\} } \lvert y-I(x \in c') \rvert^2 \quad p_{x,c}(dx\ dy)\\
    &= \int_X \lvert 0-I(x \in c') \rvert^2 \quad p_x(dx \times \{0\}) +
    \int_X \lvert 1-I(x \in c') \rvert^2 \quad p_x(dx \times \{1\})\\
    &= \int_X I(x \in c')  \quad p_x(dx \times \{0\}) +
    \int_X I(x \in (c')^\complement)  \quad p_x(dx \times \{1\})\\
    &= p_{x,c}(c' \times \{0\}) + p_{x,c}( (c')^\complement \times \{1\})
\end{align*}

\bibliographystyle{scribebibsty}
\bibliography{s782references}

\end{document}